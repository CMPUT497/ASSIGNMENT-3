@inproceedings{rei-etal-2022-cometkiwi,
  title     = {CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task},
  author    = {Rei, Ricardo and Treviso, Marcos and Guerreiro, Nuno M. and Zerva, Chrysoula and Farinha, Ana C. and Maroti, Christine and de Souza, Jos{\'e} G. C. and Glushkova, Taisiya and Alves, Duarte and Coheur, Luisa and Lavie, Alon and Martins, Andr{\'e} F. T.},
  booktitle = {Proceedings of the Seventh Conference on Machine Translation (WMT)},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates (Hybrid)},
  publisher = {Association for Computational Linguistics},
  pages     = {634--645},
  url       = {https://aclanthology.org/2022.wmt-1.60},
  doi       = {10.18653/v1/2022.wmt-1.60}
}

@inproceedings{koishekenov-etal-2023-memory,
    title = "Memory-efficient {NLLB}-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model",
    author = "Koishekenov, Yeskendir  and
      Berard, Alexandre  and
      Nikoulina, Vassilina",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.198/",
    doi = "10.18653/v1/2023.acl-long.198",
    pages = "3567--3585",
    abstract = "The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that enables the removal of up to 80{\%} of experts without further finetuning and with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics can identify language-specific experts."
}

@article{gemma_2025,
    title={Gemma 3},
    url={https://goo.gle/Gemma3Report},
    publisher={Kaggle},
    author={Gemma Team},
    year={2025}
}
@article{meconi2025llmsenses,
    title        = {Do Large Language Models Understand Word Senses?},
    author       = {Meconi, Domenico and Stirpe, Simone and Martelli, Federico and Lavalle, Leonardo and Navigli, Roberto},
    journal      = {arXiv preprint arXiv:2509.13905},
    year         = {2025},
    url          = {https://arxiv.org/pdf/2509.13905v1}
}

@inproceedings{qi2020stanza,
    title={Stanza: A Python Natural Language Processing Toolkit for Many Human Languages},
    author={Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
    booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
    pages={101--108},
    year={2020}
}

@inproceedings{dou-neubig-2021-word,
    title = "Word Alignment by Fine-tuning Embeddings on Parallel Corpora",
    author = "Dou, Zi-Yi  and
      Neubig, Graham",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.181",
    pages = "2112--2128",
}

@inproceedings{jalili-sabet-etal-2020-simalign,
    title = "{S}im{A}lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings",
    author = "Jalili Sabet, Masoud  and
      Dufter, Philipp  and
      Yvon, Fran{\c{c}}ois  and
      Sch{\"u}tze, Hinrich",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.147",
    pages = "1627--1643",
}